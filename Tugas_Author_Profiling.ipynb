{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tugas Author Profiling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_imED_vPZai7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Catatan:\n",
        "Monggo bisa nyoba nambah model di pipeline make classifier lainnya dengan parameter atau feature yang beda-beda. Parameter SVM-nya berdasarkan paper yang disitasi.\n",
        "\n",
        "File testing-nya gak ada labelnya jadi buat evaluasi akurasi pake hold out atau k-fold di training\n",
        "\n",
        "Nuwuuuuuun.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ksgxDajVzBoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Author Profiling\n",
        "A final project for KS SC: NLP class by:\n",
        "1.   Alvin Romadhon\n",
        "2.   Andri Immanudin\n",
        "3.   Desthalia\n",
        "4.   Gama Candra T. K.\n",
        "\n",
        "\n",
        "This work is an attemp to recreate work based on:\n",
        "> Basile, A., Dwyer, G., Medvedeva, M., Rawee, J., Haagsma, H., & Nissim, M. (2017). N-GrAM: New Groningen Author-profiling Model. *CoRR, abs/1707.03764.*\n",
        "\n",
        "*All files can be accessed on [this Google Drive](https://drive.google.com/drive/folders/1cbEc24TJWsw80WcXKDZ1HXB3OU8oTz9Z).*\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "QEOE-kh745dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A. Importing libraries"
      ]
    },
    {
      "metadata": {
        "id": "9_LaW4lJguzj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGv7FSzk4qJ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## B. Downloading all files"
      ]
    },
    {
      "metadata": {
        "id": "D3f8Df8Y43fu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Folder's IDs for each test and training folder\n",
        "\n",
        "nama = {'train_nl': '1g9pau51NnPanw9NUcVd1_OP7BfdYiCBh',\n",
        "        'train_en': '1A6nHq1picLrXMyNF4xMw-vFxTP2STQ6r',\n",
        "        'train_es': '162c0Szb5IffXR7jtSQUJczoV_UoXVxDy',\n",
        "        'test_nl': '15h8cy1AyJ6w-pxy7b3XD8e5aOXrLErpw',\n",
        "        'test_en': '1gHdNyh29E4qKY66Es4vkXg4Q6JYPuuPO',\n",
        "        'test_es': '1TIo79hmG5TcrYe-U1g9tXQl10aVvj4FE'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i-tW-lPJzAhx",
        "colab_type": "code",
        "outputId": "968665a1-4569-412f-c4ab-3cf8af4a6074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "cell_type": "code",
      "source": [
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Choose a local (colab) directory to store the data\n",
        "current_dir = \"~/\"\n",
        "for key, val in nama.items():\n",
        "  local_download_path = os.path.expanduser(os.path.join(current_dir, key))\n",
        "  try: \n",
        "    os.makedirs(local_download_path)\n",
        "  except: pass\n",
        "                                           \n",
        "  # 2. Auto-iterate using the query syntax\n",
        "  #    https://developers.google.com/drive/v2/web/search-parameters\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents\".format(val)}).GetList()\n",
        "\n",
        "  # 3. Create & download by ID\n",
        "  print(\"Downloading to {}\".format(local_download_path))\n",
        "  for f in file_list:\n",
        "    f_ = drive.CreateFile({'id': f['id']})\n",
        "    fname = os.path.join(local_download_path, f['title'])\n",
        "    f_.GetContentFile(fname, mimetype='text/csv')\n",
        "  print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading to /root/train_nl\n",
            "Done!\n",
            "Downloading to /root/train_en\n",
            "Done!\n",
            "Downloading to /root/train_es\n",
            "Done!\n",
            "Downloading to /root/test_nl\n",
            "Done!\n",
            "Downloading to /root/test_en\n",
            "Done!\n",
            "Downloading to /root/test_es\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XsUpf6XO5Skf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## C. Preprocessing\n",
        "\n",
        "Includes:\n",
        "1.   Combining all csvs\n",
        "2.   Removal of special characters, numbers, and link\n"
      ]
    },
    {
      "metadata": {
        "id": "aSlGjjWN5h3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Combine all CSVs from each folder\n",
        "\n",
        "#Train\n",
        "train = {}\n",
        "train_ = ['train_nl', 'train_en', 'train_es']\n",
        "\n",
        "for t in train_:  \n",
        "  path = os.path.join(\"/root/\", t)\n",
        "  files = glob.glob(path + \"/*.csv\")\n",
        "  \n",
        "  df = pd.DataFrame()\n",
        "  for f in files:\n",
        "    try:\n",
        "      tmp = pd.read_csv(f, usecols=['tgl', 'gen'])\n",
        "      df = df.append(tmp)\n",
        "    except: \n",
        "      continue\n",
        "  train[t] = df.reset_index(drop = True)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_JEzNv_5kgvM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Remove mentions, punctuations, link, hashtag, and numbers\n",
        "for t in train:\n",
        "  train[t]['tgl_bersih'] = train[t]['tgl'].str.replace(\n",
        "      r'(b\\\")|(b\\')|(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(\\w*\\d\\w*)', \n",
        "      '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJV5z7XMEsi8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## D. Feature Extraction and Modeling\n",
        "Some preprocessing steps were also done in this section, mainly tokenizing and case folding. For feature extraction, we used two feature vectors: character n-gram (n = 3-5) and word n-gram. The classifier we used is Linear SVM."
      ]
    },
    {
      "metadata": {
        "id": "v60RBg_ItlkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svm = Pipeline([('features', FeatureUnion([\n",
        "    ('char',TfidfVectorizer(ngram_range=(3,6), analyzer = 'char', min_df=2,\n",
        "                            lowercase=True, sublinear_tf=True, use_idf=True,\n",
        "                           norm='l2', binary = False)),\n",
        "    ('word', TfidfVectorizer(analyzer = 'word', lowercase=True, min_df=2, \n",
        "                             sublinear_tf=True, norm='l2', binary = False))\n",
        "    ])),\n",
        "    ('clf', LinearSVC(C=1.0))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Rj137svSkfK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "rfc = Pipeline([('features', FeatureUnion([\n",
        "    ('char',TfidfVectorizer(ngram_range=(3,6), analyzer = 'char', min_df=2,\n",
        "                            lowercase=True, sublinear_tf=True, use_idf=True,\n",
        "                           norm='l2', binary = False)),\n",
        "    ('word', TfidfVectorizer(analyzer = 'word', lowercase=True, min_df=2, \n",
        "                             sublinear_tf=True, norm='l2', binary = False))\n",
        "    ])),\n",
        "    ('clf', RandomForestClassifier())])\n",
        "\n",
        "abc = Pipeline([('features', FeatureUnion([\n",
        "    ('char',TfidfVectorizer(ngram_range=(3,6), analyzer = 'char', min_df=2,\n",
        "                            lowercase=True, sublinear_tf=True, use_idf=True,\n",
        "                           norm='l2', binary = False)),\n",
        "    ('word', TfidfVectorizer(analyzer = 'word', lowercase=True, min_df=2, \n",
        "                             sublinear_tf=True, norm='l2', binary = False))\n",
        "    ])),\n",
        "    ('clf', AdaBoostClassifier())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jysE7mtrbjHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = Pipeline([('features', FeatureUnion([\n",
        "    ('char',TfidfVectorizer(ngram_range=(3,6), analyzer = 'char', min_df=2,\n",
        "                            lowercase=True, sublinear_tf=True, use_idf=True,\n",
        "                           norm='l2', binary = False)),\n",
        "    ('word', TfidfVectorizer(analyzer = 'word', lowercase=True, min_df=2, \n",
        "                             sublinear_tf=True, norm='l2', binary = False))\n",
        "    ])),\n",
        "    ('clf', GaussianNB())])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "teRKykoXwKil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## E. Training and Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "A3jhxC4eySM8",
        "colab_type": "code",
        "outputId": "0e8f1487-194f-4689-9479-ea9a105fe3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# SVM\n",
        "for t in train:\n",
        "  X_train = train[t]['tgl_bersih']\n",
        "  y_train = train[t]['gen']\n",
        "#   pipeline.fit(X_train, y_train)\n",
        "  score = cross_val_score(svm, X_train, y_train, cv=5).mean()\n",
        "  \n",
        "  print(\"Score {}: {}\".format(t, score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score train_nl: 0.5495080251763877\n",
            "Score train_en: 0.6053316553197199\n",
            "Score train_es: 0.5566900442539706\n",
            "CPU times: user 8min 46s, sys: 3.94 s, total: 8min 50s\n",
            "Wall time: 8min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0j9-1j59S8_c",
        "colab_type": "code",
        "outputId": "50882fca-86b6-460e-c996-87c7b87b9e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# RFC\n",
        "for t in train:\n",
        "  X_train = train[t]['tgl_bersih']\n",
        "  y_train = train[t]['gen']\n",
        "#   pipeline.fit(X_train, y_train)\n",
        "  score = cross_val_score(rfc, X_train, y_train, cv=5).mean()\n",
        "  \n",
        "  print(\"Score {}: {}\".format(t, score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score train_nl: 0.5440745670647903\n",
            "Score train_en: 0.5818055448843391\n",
            "Score train_es: 0.54053709816234\n",
            "CPU times: user 43min 35s, sys: 3.02 s, total: 43min 38s\n",
            "Wall time: 43min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lr-TXvNWUiF3",
        "colab_type": "code",
        "outputId": "8a11c1b7-7dee-458c-8710-3e1aa98f77d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# adaboost\n",
        "for t in train:\n",
        "  X_train = train[t]['tgl_bersih']\n",
        "  y_train = train[t]['gen']\n",
        "#   pipeline.fit(X_train, y_train)\n",
        "  score = cross_val_score(abc, X_train, y_train, cv=5).mean()\n",
        "  \n",
        "  print(\"Score {}: {}\".format(t, score))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score train_nl: 0.5452319837656497\n",
            "Score train_en: 0.5728941259633\n",
            "Score train_es: 0.5706681313266369\n",
            "CPU times: user 47min 34s, sys: 4.62 s, total: 47min 38s\n",
            "Wall time: 47min 39s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bd--ZqDKHFOu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Kesimpulannya, bahwa SVM dari segi running time beserta akurasi jauh lebih akurat dan lebih sedikit running timenya"
      ]
    },
    {
      "metadata": {
        "id": "618Ul7kvyOio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## F. Testing"
      ]
    },
    {
      "metadata": {
        "id": "5FXNc9LNKL7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "predict = {}\n",
        "test_ = ['test_nl', 'test_en', 'test_es']\n",
        "\n",
        "for e in test_:\n",
        "  path = os.path.join(\"/root/\", e)\n",
        "  files = glob.glob(path + \"/*.csv\")\n",
        "  \n",
        "  for f in files:\n",
        "    try:\n",
        "      df = pd.read_csv(f)\n",
        "      X_test = df[df.columns[1]]\n",
        "      \n",
        "      predict[x] = svm.predict(X_test)\n",
        "    except: \n",
        "      continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSEcdN2QNvfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "path = os.path.join(\"/root/\", \"test_en\")\n",
        "files_en = glob.glob(path + \"/*.csv\")\n",
        "predict_en = []\n",
        "\n",
        "for f in files_en:\n",
        "  try:\n",
        "    df = pd.read_csv(f)\n",
        "    X_test = df[df.columns[1]]\n",
        "\n",
        "    predict_en.append(svm.predict(X_test))\n",
        "  except:\n",
        "    continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7DAqPldRU1hd",
        "colab_type": "code",
        "outputId": "7be26b0e-0153-4961-82da-95483f352f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "predict_en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "bak7HmIKU5Wn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}